{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62decc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan data loaded successfully.\n",
      "Unemployment rate data loaded and merged successfully.\n",
      "Loan and unemployment data merged successfully.\n",
      "Data preprocessing completed successfully.\n",
      "\n",
      " Outlier Clipping Summary (|z| > 4):\n",
      "                            num_clipped\n",
      "application_type                 120710\n",
      "disbursement_method               78122\n",
      "collections_12_mths_ex_med        37438\n",
      "last_fico_range_low               37326\n",
      "debt_settlement_flag              34246\n",
      "num_accts_ever_120_pd             32592\n",
      "delinq_2yrs                       29347\n",
      "recoveries                        27522\n",
      "collection_recovery_fee           26637\n",
      "mo_sin_rcnt_rev_tl_op             26342\n",
      "last_pymnt_amnt                   25573\n",
      "bc_open_to_buy                    23282\n",
      "total_rec_int                     22189\n",
      "mths_since_recent_bc              21542\n",
      "tax_liens                         20992\n",
      "total_bal_ex_mort                 20445\n",
      "mo_sin_rcnt_tl                    19822\n",
      "mths_since_rcnt_il                19105\n",
      "total_bc_limit                    18460\n",
      "revol_bal                         18450\n",
      "\n",
      " Total rows dropped: 703748 out of 2260668 (31.1301% of data)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from data_preprocessing import preprocess_data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "loan_data = \"data/accepted_2007_to_2018Q4.csv.gz\"\n",
    "unemployment_rate_data = [\"data/unemployment_rate_0.csv\", \"data/unemployment_rate_1.csv\", \"data/unemployment_rate_2.csv\", \"data/unemployment_rate_3.csv\", \"data/unemployment_rate_4.csv\"]\n",
    "\n",
    "data = preprocess_data(loan_data, unemployment_rate_data)\n",
    "\n",
    "X = data.drop(columns=['default']).copy()\n",
    "y = data['default'].copy()\n",
    "\n",
    "\n",
    "# Compute Z-scores\n",
    "# z = (X - X.mean()) / X.std(ddof=0)\n",
    "\n",
    "# # Track clipping\n",
    "# clipped_counts = {}\n",
    "# rows_to_drop = pd.Series(False, index=X.index)  # track all rows to drop\n",
    "\n",
    "# for col in X.columns:\n",
    "#     high_mask = z[col] > 4.0\n",
    "#     low_mask = z[col] < -4.0\n",
    "#     total_clipped = high_mask.sum() + low_mask.sum()\n",
    "\n",
    "#     if total_clipped > 0:\n",
    "#         clipped_counts[col] = total_clipped\n",
    "#         # Mark rows for dropping instead of replacing values\n",
    "#         rows_to_drop = rows_to_drop | high_mask | low_mask\n",
    "\n",
    "# # Drop all rows with any outlier after checking all columns\n",
    "# X_clipped = X[~rows_to_drop]\n",
    "# y_clipped = y[~rows_to_drop]  # Make sure target matches\n",
    "\n",
    "# # Reporting\n",
    "# if clipped_counts:\n",
    "#     outlier_report = pd.DataFrame.from_dict(clipped_counts, orient='index', columns=['num_clipped'])\n",
    "#     outlier_report = outlier_report.sort_values(by='num_clipped', ascending=False)\n",
    "#     print(\"\\n Outlier Clipping Summary (|z| > 4):\")\n",
    "#     print(outlier_report.head(20))\n",
    "#     total_clipped = rows_to_drop.sum()\n",
    "#     print(f\"\\n Total rows dropped: {total_clipped} out of {len(X)} \"\n",
    "#           f\"({(total_clipped / len(X)) * 100:.4f}% of data)\")\n",
    "# else:\n",
    "#     print(\"No outliers detected beyond |z| > 4\")\n",
    "\n",
    "# # then impute (if needed) and scale\n",
    "# imputer = SimpleImputer(strategy='median')\n",
    "# X_imp = imputer.fit_transform(X_clipped)\n",
    "# X_scaled = StandardScaler().fit_transform(X_imp)\n",
    "\n",
    "# # Number of components PCA\n",
    "# n = 20\n",
    "# pca = PCA(n_components=n)\n",
    "\n",
    "# # Fit and transform\n",
    "# X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "import numpy as np\n",
    "loading_strength = np.abs(pca.components_[0])  # first PC\n",
    "top_features = np.argsort(loading_strength)[::-1][:10]  # top 10 features\n",
    "print(\"Top 10 features contributing to the first principal component:\")\n",
    "print(data.columns[top_features])\n",
    "\n",
    "#K-means\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters=5, n_init='auto', random_state=0)\n",
    "labels = km.fit_predict(X_pca)\n",
    "\n",
    "\n",
    "# Add cluster labels to the original data\n",
    "data['cluster'] = labels\n",
    "\n",
    "data['cluster'].value_counts()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "riskBorrow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
