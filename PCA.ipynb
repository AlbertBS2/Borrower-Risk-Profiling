{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f197c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan data loaded successfully.\n",
      "Unemployment rate data loaded and merged successfully.\n",
      "Loan and unemployment data merged successfully.\n",
      "Data preprocessing completed successfully.\n",
      "Numeric feature count: 97\n",
      "10 Explained variance ratio \n",
      "[0.11801343 0.0765982  0.0699868  0.0541901  0.04118066 0.04015553\n",
      " 0.02884461 0.02664976 0.02577038 0.02092239]\n",
      "10 Cum variance\n",
      "[0.11801343 0.19461163 0.26459844 0.31878853 0.3599692  0.40012473\n",
      " 0.42896934 0.4556191  0.48138948 0.50231187]\n",
      "Print all features by PCA importance:\n",
      "                       feature  importance\n",
      "0                num_rev_accts    0.078431\n",
      "1         mo_sin_old_rev_tl_op    0.077408\n",
      "2                    num_bc_tl    0.076253\n",
      "3                total_rec_int    0.075616\n",
      "4            total_bal_ex_mort    0.075081\n",
      "5                    total_acc    0.074716\n",
      "6              last_pymnt_amnt    0.074224\n",
      "7              tot_hi_cred_lim    0.074198\n",
      "8              total_rec_prncp    0.074169\n",
      "9                    revol_bal    0.074072\n",
      "10                 installment    0.073717\n",
      "11             num_actv_rev_tl    0.073715\n",
      "12          num_tl_op_past_12m    0.073640\n",
      "13              home_ownership    0.073609\n",
      "14                 tot_cur_bal    0.073569\n",
      "15         num_rev_tl_bal_gt_0    0.073512\n",
      "16                   num_il_tl    0.073231\n",
      "17            total_rev_hi_lim    0.072667\n",
      "18  total_il_high_credit_limit    0.072602\n",
      "19              total_bc_limit    0.072486\n",
      "20        acc_open_past_24mths    0.071542\n",
      "21             total_pymnt_inv    0.071495\n",
      "22                 total_pymnt    0.071455\n",
      "23          mo_sin_old_il_acct    0.071372\n",
      "24              bc_open_to_buy    0.071036\n",
      "25        mths_since_recent_bc    0.070990\n",
      "26              mo_sin_rcnt_tl    0.070924\n",
      "27              num_actv_bc_tl    0.070257\n",
      "28                 open_acc_6m    0.070227\n",
      "29       mo_sin_rcnt_rev_tl_op    0.070159\n",
      "30                        term    0.069956\n",
      "31             fico_range_high    0.069719\n",
      "32              fico_range_low    0.069719\n",
      "33                 open_rv_12m    0.069327\n",
      "34                inq_last_12m    0.069283\n",
      "35                 avg_cur_bal    0.069141\n",
      "36                 open_rv_24m    0.068983\n",
      "37                    num_sats    0.068789\n",
      "38                    open_acc    0.068629\n",
      "39                    mort_acc    0.068381\n",
      "40                     bc_util    0.068274\n",
      "41            percent_bc_gt_75    0.068109\n",
      "42         last_fico_range_low    0.068097\n",
      "43                    all_util    0.067760\n",
      "44       num_accts_ever_120_pd    0.067647\n",
      "45                 num_bc_sats    0.067616\n",
      "46        last_fico_range_high    0.067188\n",
      "47                      inq_fi    0.066483\n",
      "48                   loan_amnt    0.066473\n",
      "49                 funded_amnt    0.066446\n",
      "50               num_op_rev_tl    0.066419\n",
      "51             funded_amnt_inv    0.066296\n",
      "52              pct_tl_nvr_dlq    0.066188\n",
      "53                 open_il_24m    0.065696\n",
      "54          num_tl_90g_dpd_24m    0.065110\n",
      "55                   out_prncp    0.064877\n",
      "56               out_prncp_inv    0.064877\n",
      "57              inq_last_6mths    0.064806\n",
      "58                         dti    0.064653\n",
      "59                  issue_year    0.064300\n",
      "60       mths_since_recent_inq    0.064209\n",
      "61                 open_act_il    0.064006\n",
      "62                  revol_util    0.063780\n",
      "63                  annual_inc    0.063666\n",
      "64                total_bal_il    0.063370\n",
      "65        pub_rec_bankruptcies    0.063090\n",
      "66                 delinq_2yrs    0.063033\n",
      "67          mths_since_rcnt_il    0.062916\n",
      "68                 open_il_12m    0.062354\n",
      "69                  max_bal_bc    0.061688\n",
      "70                     il_util    0.061666\n",
      "71                 total_cu_tl    0.061549\n",
      "72                     purpose    0.061402\n",
      "73                  emp_length    0.060774\n",
      "74         y_unemployment_rate    0.060595\n",
      "75                    int_rate    0.056278\n",
      "76     collection_recovery_fee    0.056199\n",
      "77                       grade    0.056053\n",
      "78                  recoveries    0.056025\n",
      "79                   sub_grade    0.055683\n",
      "80         verification_status    0.055293\n",
      "81                   tax_liens    0.053716\n",
      "82    chargeoff_within_12_mths    0.050511\n",
      "83        debt_settlement_flag    0.049110\n",
      "84                     pub_rec    0.049034\n",
      "85            application_type    0.048973\n",
      "86          total_rec_late_fee    0.044900\n",
      "87                num_tl_30dpd    0.043933\n",
      "88         disbursement_method    0.043036\n",
      "89                  addr_state    0.042692\n",
      "90  collections_12_mths_ex_med    0.042512\n",
      "91            num_tl_120dpd_2m    0.041368\n",
      "92              acc_now_delinq    0.041025\n",
      "93                 delinq_amnt    0.039585\n",
      "94                tot_coll_amt    0.024679\n",
      "95               hardship_flag    0.019965\n",
      "96                  pymnt_plan    0.019954\n",
      "Loadings for top features (first 5 PCs):\n",
      "                           PC1       PC2       PC3       PC4       PC5\n",
      "num_rev_accts         0.198437 -0.116012 -0.027075 -0.131971 -0.043615\n",
      "mo_sin_old_rev_tl_op  0.087752  0.055848 -0.051311 -0.035915 -0.052181\n",
      "num_bc_tl             0.191210 -0.084297 -0.046638 -0.150265 -0.042759\n",
      "total_rec_int         0.107931  0.151286  0.183061 -0.066705  0.086825\n",
      "total_bal_ex_mort     0.160530  0.052221  0.033801  0.217307 -0.077069\n",
      "total_acc             0.207763 -0.081511  0.010582  0.056269 -0.014970\n",
      "last_pymnt_amnt       0.080547  0.102412  0.052284 -0.058177  0.211776\n",
      "tot_hi_cred_lim       0.168610  0.108415 -0.047439  0.149361 -0.006707\n",
      "total_rec_prncp       0.129892  0.191147  0.071100 -0.126073  0.236529\n",
      "revol_bal             0.146160  0.102448 -0.004461 -0.016314 -0.148026\n",
      "Number of components to reach 95% variance: 51\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from data_preprocessing import preprocess_data\n",
    "\n",
    "loan_data = \"data/accepted_2007_to_2018Q4.csv.gz\"\n",
    "unemployment_rate_data = [\"data/unemployment_rate_0.csv\", \"data/unemployment_rate_1.csv\", \"data/unemployment_rate_2.csv\", \"data/unemployment_rate_3.csv\", \"data/unemployment_rate_4.csv\"]\n",
    "\n",
    "data = preprocess_data(loan_data, unemployment_rate_data)\n",
    "\n",
    "# Drop non-numeric columns and the target column `default` from features (but keep for reference)\n",
    "if 'default' in data.columns:\n",
    "    y = data['default']\n",
    "else:\n",
    "    y = None\n",
    "numeric_columns = data.columns[data.dtypes.apply(lambda x: np.issubdtype(x, np.number))]\n",
    "\n",
    "# ISSUES WITH MEMORY USAGE\n",
    "data = data[numeric_columns] \n",
    "# non_numeric = [c for c in data.columns if c not in numeric_columns]\n",
    "# data.drop(columns=non_numeric, inplace=True)\n",
    "\n",
    "# Remove IDs\n",
    "for col in ['id','member_id']:\n",
    "    if col in data.columns:\n",
    "        data = data.drop(columns=[col])\n",
    "# Ensure target not included in features\n",
    "if 'default' in data.columns:\n",
    "    data = data.drop(columns=['default'])\n",
    "\n",
    "print(f'Numeric feature count: {data.shape[1]}')\n",
    "\n",
    "# Impute missing values with median\n",
    "imp = SimpleImputer(strategy='median')\n",
    "X_imp = imp.fit_transform(data)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "# Run PCA\n",
    "batch_size = 50000\n",
    "for i in range(0, X_imp.shape[0], batch_size):\n",
    "    scaler.partial_fit(X_imp[i:i+batch_size])\n",
    "\n",
    "# Transform in batches\n",
    "X_scaled = np.zeros_like(X_imp, dtype=np.float32)  # Saves memory vs float64\n",
    "for i in range(0, X_imp.shape[0], batch_size):\n",
    "    X_scaled[i:i+batch_size] = scaler.transform(X_imp[i:i+batch_size])\n",
    "\n",
    "n_components = min(50, X_scaled.shape[1])  # Choose desired number of components (50 is typical)\n",
    "ipca = IncrementalPCA(n_components=n_components)\n",
    "\n",
    "# Fit in chunks\n",
    "for i in range(0, X_scaled.shape[0], batch_size):\n",
    "    ipca.partial_fit(X_scaled[i:i+batch_size])\n",
    "\n",
    "# Transform in chunks\n",
    "X_pca_list = []\n",
    "for i in range(0, X_scaled.shape[0], batch_size):\n",
    "    X_pca_list.append(ipca.transform(X_scaled[i:i+batch_size]))\n",
    "\n",
    "X_pca = np.vstack(X_pca_list)\n",
    "\n",
    "# Explained variance\n",
    "explained = ipca.explained_variance_ratio_\n",
    "cum_explained = np.cumsum(explained)\n",
    "print('10 Explained variance ratio ')\n",
    "print(explained[:10])\n",
    "print('10 Cum variance')\n",
    "print(cum_explained[:10])\n",
    "\n",
    "# Loadings DataFrame (features x components)\n",
    "loadings = pd.DataFrame(ipca.components_.T, index=data.columns, columns=[f'PC{i+1}' for i in range(ipca.n_components_)])\n",
    "importance = (loadings.abs() * explained).sum(axis=1)\n",
    "importance = importance.sort_values(ascending=False)\n",
    "impl_df = pd.DataFrame({'feature': importance.index, 'importance': importance.values})\n",
    "impl_df.to_csv('pca_feature_importance.csv', index=False)\n",
    "\n",
    "#print('Top 20 features by PCA importance:')\n",
    "# print(impl_df.head(20))\n",
    "print('Print all features by PCA importance to -> pca_feature_importance.csv:')\n",
    "\n",
    "top_feats = impl_df['feature'].head(10).tolist()\n",
    "print('Loadings for top features (first 5 PCs):')\n",
    "print(loadings.loc[top_feats, loadings.columns[:5]])\n",
    "\n",
    "# Components to reach 95% variance\n",
    "n_95 = np.searchsorted(cum_explained, 0.95) + 1\n",
    "print(f'Number of components to reach 95% variance: {n_95}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc03c76d",
   "metadata": {},
   "source": [
    "### Logistic Regression on 50 features with highest variance (according to PCA)\n",
    "\n",
    "Note, the PCA is a bit more complicated than it needs to be. As it had to be done batch wise to perserve memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8756bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 50 components for model input\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniel\\anaconda3\\envs\\riskBorrow\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics:\n",
      "{'accuracy': 0.9592886179760868, 'precision': 0.8039651070578906, 'recall': 0.9205712398877332, 'f1': 0.858325957283048, 'roc_auc': 0.9842582346650408}\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98    391564\n",
      "           1       0.80      0.92      0.86     60570\n",
      "\n",
      "    accuracy                           0.96    452134\n",
      "   macro avg       0.90      0.94      0.92    452134\n",
      "weighted avg       0.96      0.96      0.96    452134\n",
      "\n",
      "Confusion matrix:\n",
      "[[377968  13596]\n",
      " [  4811  55759]]\n",
      "Saved metrics to pca_logistic_metrics.csv and coefficients to pca_logistic_coefficients.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "desired = 51\n",
    "if 'n_95' in globals() and isinstance(n_95, int) and n_95 > 0:\n",
    "    n_use = min(desired, ipca.n_components_, n_95)\n",
    "else:\n",
    "    n_use = min(desired, ipca.n_components_)\n",
    "\n",
    "print(f'Using {n_use} components for model input')\n",
    "\n",
    "# Build feature matrix from PCA-transformed data X_pca\n",
    "if 'X_pca' not in globals():\n",
    "    X_pca = ipca.transform(X_scaled)\n",
    "X = X_pca[:, :n_use]\n",
    "\n",
    "try:\n",
    "    y = data['default'] \n",
    "except Exception:\n",
    "    # fall back to checking globals for y from earlier cells\n",
    "    if 'y' in globals() and y is not None:\n",
    "        pass\n",
    "    else:\n",
    "        raise RuntimeError('Target vector `y` not found in notebook namespace. Ensure you saved the original target before overwriting `data`.')\n",
    "\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "# Fit logistic regression with balanced class weights\n",
    "clf = LogisticRegression(max_iter=1000, class_weight='balanced', solver='saga')\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:, 1] if hasattr(clf, 'predict_proba') else None\n",
    "\n",
    "\n",
    "# Metrics\n",
    "metrics = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred),\n",
    "    'precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "    'recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "    'f1': f1_score(y_test, y_pred, zero_division=0),\n",
    "}\n",
    "\n",
    "if y_proba is not None:\n",
    "    metrics['roc_auc'] = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "print('Evaluation metrics:')\n",
    "print(metrics)\n",
    "\n",
    "print('Classification report:')\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Save metrics and model coefficients\n",
    "metrics_df = pd.DataFrame([metrics])\n",
    "metrics_df.to_csv('pca_logistic_metrics.csv', index=False)\n",
    "coef_df = pd.DataFrame({'component': [f'PC{i+1}' for i in range(n_use)], 'coef': clf.coef_.ravel()[:n_use]})\n",
    "coef_df.to_csv('pca_logistic_coefficients.csv', index=False)\n",
    "print('Saved metrics to pca_logistic_metrics.csv and coefficients to pca_logistic_coefficients.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72577c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan data loaded successfully.\n",
      "Unemployment rate data loaded and merged successfully.\n",
      "Loan and unemployment data merged successfully.\n",
      "Data preprocessing completed successfully.\n",
      "\n",
      "Top 20 Features by Point-Biserial Correlation:\n",
      "                    feature  abs_pointbiserial_corr\n",
      "35     last_fico_range_high                0.609548\n",
      "36      last_fico_range_low                0.556323\n",
      "32               recoveries                0.488174\n",
      "33  collection_recovery_fee                0.463735\n",
      "94     debt_settlement_flag                0.314878\n",
      "7                 sub_grade                0.233739\n",
      "29          total_rec_prncp                0.233454\n",
      "6                     grade                0.229600\n",
      "4                  int_rate                0.211744\n",
      "34          last_pymnt_amnt                0.192632\n",
      "25                out_prncp                0.157300\n",
      "26            out_prncp_inv                0.157285\n",
      "27              total_pymnt                0.145531\n",
      "28          total_pymnt_inv                0.145372\n",
      "31       total_rec_late_fee                0.144559\n",
      "17           fico_range_low                0.122063\n",
      "18          fico_range_high                0.122062\n",
      "95               issue_year                0.112514\n",
      "11      verification_status                0.095424\n",
      "57     acc_open_past_24mths                0.094126\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pointbiserialr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "loan_data = \"data/accepted_2007_to_2018Q4.csv.gz\"\n",
    "unemployment_rate_data = [\"data/unemployment_rate_0.csv\", \"data/unemployment_rate_1.csv\", \"data/unemployment_rate_2.csv\", \"data/unemployment_rate_3.csv\", \"data/unemployment_rate_4.csv\"]\n",
    "\n",
    "data = preprocess_data(loan_data, unemployment_rate_data)\n",
    "target = data['default']\n",
    "features = data.drop(columns=['default'])\n",
    "numeric_cols = features.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "correlation_scores = []\n",
    "\n",
    "for col in numeric_cols:\n",
    "    try:\n",
    "        # Handle missing values in column by filling with median\n",
    "        series = features[col].fillna(features[col].median())\n",
    "        r, _ = pointbiserialr(target, series)\n",
    "        correlation_scores.append((col, abs(r)))\n",
    "    except Exception as e:\n",
    "        print(f\"Skipped {col} due to error: {e}\")\n",
    "\n",
    "corr_df = pd.DataFrame(correlation_scores, columns=['feature', 'abs_pointbiserial_corr'])\n",
    "corr_df = corr_df.sort_values('abs_pointbiserial_corr', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 Features by Point-Biserial Correlation:\")\n",
    "print(corr_df.head(20))\n",
    "\n",
    "# Save for audit\n",
    "corr_df.to_csv(\"pointbiserial_correlation_scores.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "riskBorrow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
