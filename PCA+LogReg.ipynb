{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f197c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan data loaded successfully.\n",
      "Unemployment rate data loaded and merged successfully.\n",
      "Loan and unemployment data merged successfully.\n",
      "Data preprocessing completed successfully.\n",
      "Initial data shape: (2260668, 98)\n",
      "Numeric feature count: 97\n",
      "10 Explained variance ratio \n",
      "[0.11801343 0.0765982  0.0699868  0.0541901  0.04118066 0.04015553\n",
      " 0.02884461 0.02664976 0.02577038 0.02092239]\n",
      "10 Cum variance\n",
      "[0.11801343 0.19461163 0.26459844 0.31878853 0.3599692  0.40012473\n",
      " 0.42896934 0.4556191  0.48138948 0.50231187]\n",
      "Print all features by PCA importance to -> pca_feature_importance.csv:\n",
      "Loadings for top features (first 5 PCs):\n",
      "                           PC1       PC2       PC3       PC4       PC5\n",
      "num_rev_accts         0.198437 -0.116012 -0.027075 -0.131971 -0.043615\n",
      "mo_sin_old_rev_tl_op  0.087752  0.055848 -0.051311 -0.035915 -0.052181\n",
      "num_bc_tl             0.191210 -0.084297 -0.046638 -0.150265 -0.042759\n",
      "total_rec_int         0.107931  0.151286  0.183061 -0.066705  0.086825\n",
      "total_bal_ex_mort     0.160530  0.052221  0.033801  0.217307 -0.077069\n",
      "total_acc             0.207763 -0.081511  0.010582  0.056269 -0.014970\n",
      "last_pymnt_amnt       0.080547  0.102412  0.052284 -0.058177  0.211776\n",
      "tot_hi_cred_lim       0.168610  0.108415 -0.047439  0.149361 -0.006707\n",
      "total_rec_prncp       0.129892  0.191147  0.071100 -0.126073  0.236529\n",
      "revol_bal             0.146160  0.102448 -0.004461 -0.016314 -0.148026\n",
      "Number of components to reach 95% variance: 51\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from data_preprocessing import preprocess_data\n",
    "\n",
    "loan_data = \"data/accepted_2007_to_2018Q4.csv.gz\"\n",
    "unemployment_rate_data = [\"data/unemployment_rate_0.csv\", \"data/unemployment_rate_1.csv\", \"data/unemployment_rate_2.csv\", \"data/unemployment_rate_3.csv\", \"data/unemployment_rate_4.csv\"]\n",
    "\n",
    "data = preprocess_data(loan_data, unemployment_rate_data)\n",
    "print(f'Initial data shape: {data.shape}')\n",
    "print(data.to_string())\n",
    "\n",
    "# Drop non-numeric columns and the target column `default` from features (but keep for reference)\n",
    "if 'default' in data.columns:\n",
    "    y = data['default']\n",
    "else:\n",
    "    y = None\n",
    "numeric_columns = data.columns[data.dtypes.apply(lambda x: np.issubdtype(x, np.number))]\n",
    "\n",
    "# ISSUES WITH MEMORY USAGE\n",
    "data = data[numeric_columns] \n",
    "# non_numeric = [c for c in data.columns if c not in numeric_columns]\n",
    "# data.drop(columns=non_numeric, inplace=True)\n",
    "\n",
    "# Remove IDs\n",
    "for col in ['id','member_id']:\n",
    "    if col in data.columns:\n",
    "        data = data.drop(columns=[col])\n",
    "# Ensure target not included in features\n",
    "if 'default' in data.columns:\n",
    "    data = data.drop(columns=['default'])\n",
    "\n",
    "print(f'Numeric feature count: {data.shape[1]}')\n",
    "\n",
    "# Impute missing values with median\n",
    "imp = SimpleImputer(strategy='median')\n",
    "X_imp = imp.fit_transform(data)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "# Run PCA\n",
    "batch_size = 50000\n",
    "for i in range(0, X_imp.shape[0], batch_size):\n",
    "    scaler.partial_fit(X_imp[i:i+batch_size])\n",
    "\n",
    "# Transform in batches\n",
    "X_scaled = np.zeros_like(X_imp, dtype=np.float32)  # Saves memory vs float64\n",
    "for i in range(0, X_imp.shape[0], batch_size):\n",
    "    X_scaled[i:i+batch_size] = scaler.transform(X_imp[i:i+batch_size])\n",
    "\n",
    "n_components = min(50, X_scaled.shape[1])  # Choose desired number of components (50 is typical)\n",
    "ipca = IncrementalPCA(n_components=n_components)\n",
    "\n",
    "# Fit in chunks\n",
    "for i in range(0, X_scaled.shape[0], batch_size):\n",
    "    ipca.partial_fit(X_scaled[i:i+batch_size])\n",
    "\n",
    "# Transform in chunks\n",
    "X_pca_list = []\n",
    "for i in range(0, X_scaled.shape[0], batch_size):\n",
    "    X_pca_list.append(ipca.transform(X_scaled[i:i+batch_size]))\n",
    "\n",
    "X_pca = np.vstack(X_pca_list)\n",
    "\n",
    "# Explained variance\n",
    "explained = ipca.explained_variance_ratio_\n",
    "cum_explained = np.cumsum(explained)\n",
    "print('10 Explained variance ratio ')\n",
    "print(explained[:10])\n",
    "print('10 Cum variance')\n",
    "print(cum_explained[:10])\n",
    "\n",
    "# Loadings DataFrame (features x components)\n",
    "loadings = pd.DataFrame(ipca.components_.T, index=data.columns, columns=[f'PC{i+1}' for i in range(ipca.n_components_)])\n",
    "importance = (loadings.abs() * explained).sum(axis=1)\n",
    "importance = importance.sort_values(ascending=False)\n",
    "impl_df = pd.DataFrame({'feature': importance.index, 'importance': importance.values})\n",
    "impl_df.to_csv('pca_feature_importance.csv', index=False)\n",
    "\n",
    "#print('Top 20 features by PCA importance:')\n",
    "# print(impl_df.head(20))\n",
    "print('Print all features by PCA importance to -> pca_feature_importance.csv:')\n",
    "\n",
    "top_feats = impl_df['feature'].head(10).tolist()\n",
    "print('Loadings for top features (first 5 PCs):')\n",
    "print(loadings.loc[top_feats, loadings.columns[:5]])\n",
    "\n",
    "# Components to reach 95% variance\n",
    "n_95 = np.searchsorted(cum_explained, 0.95) + 1\n",
    "print(f'Number of components to reach 95% variance: {n_95}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc03c76d",
   "metadata": {},
   "source": [
    "### Logistic Regression on 50 features with highest variance (according to PCA)\n",
    "\n",
    "Note, the PCA is a bit more complicated than it needs to be. As it had to be done batch wise to perserve memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8756bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 50 components for model input\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniel\\anaconda3\\envs\\riskBorrow\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics:\n",
      "{'accuracy': 0.9592886179760868, 'precision': 0.8039651070578906, 'recall': 0.9205712398877332, 'f1': 0.858325957283048, 'roc_auc': 0.9842582346650408}\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98    391564\n",
      "           1       0.80      0.92      0.86     60570\n",
      "\n",
      "    accuracy                           0.96    452134\n",
      "   macro avg       0.90      0.94      0.92    452134\n",
      "weighted avg       0.96      0.96      0.96    452134\n",
      "\n",
      "Confusion matrix:\n",
      "[[377968  13596]\n",
      " [  4811  55759]]\n",
      "Saved metrics to pca_logistic_metrics.csv and coefficients to pca_logistic_coefficients.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "desired = 51\n",
    "if 'n_95' in globals() and isinstance(n_95, int) and n_95 > 0:\n",
    "    n_use = min(desired, ipca.n_components_, n_95)\n",
    "else:\n",
    "    n_use = min(desired, ipca.n_components_)\n",
    "\n",
    "print(f'Using {n_use} components for model input')\n",
    "\n",
    "# Build feature matrix from PCA-transformed data X_pca\n",
    "if 'X_pca' not in globals():\n",
    "    X_pca = ipca.transform(X_scaled)\n",
    "X = X_pca[:, :n_use]\n",
    "\n",
    "try:\n",
    "    y = data['default'] \n",
    "except Exception:\n",
    "    # fall back to checking globals for y from earlier cells\n",
    "    if 'y' in globals() and y is not None:\n",
    "        pass\n",
    "    else:\n",
    "        raise RuntimeError('Target vector `y` not found in notebook namespace. Ensure you saved the original target before overwriting `data`.')\n",
    "\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "# Fit logistic regression with balanced class weights\n",
    "clf = LogisticRegression(max_iter=1000, class_weight='balanced', solver='saga')\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:, 1] if hasattr(clf, 'predict_proba') else None\n",
    "\n",
    "\n",
    "# Metrics\n",
    "metrics = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred),\n",
    "    'precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "    'recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "    'f1': f1_score(y_test, y_pred, zero_division=0),\n",
    "}\n",
    "\n",
    "if y_proba is not None:\n",
    "    metrics['roc_auc'] = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "print('Evaluation metrics:')\n",
    "print(metrics)\n",
    "\n",
    "print('Classification report:')\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Save metrics and model coefficients\n",
    "metrics_df = pd.DataFrame([metrics])\n",
    "metrics_df.to_csv('pca_logistic_metrics.csv', index=False)\n",
    "coef_df = pd.DataFrame({'component': [f'PC{i+1}' for i in range(n_use)], 'coef': clf.coef_.ravel()[:n_use]})\n",
    "coef_df.to_csv('pca_logistic_coefficients.csv', index=False)\n",
    "print('Saved metrics to pca_logistic_metrics.csv and coefficients to pca_logistic_coefficients.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72577c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan data loaded successfully.\n",
      "Unemployment rate data loaded and merged successfully.\n",
      "Loan and unemployment data merged successfully.\n",
      "Data preprocessing completed successfully.\n",
      "\n",
      "Top 20 Features by Point-Biserial Correlation:\n",
      "                    feature  abs_pointbiserial_corr\n",
      "35     last_fico_range_high                0.609548\n",
      "36      last_fico_range_low                0.556323\n",
      "32               recoveries                0.488174\n",
      "33  collection_recovery_fee                0.463735\n",
      "94     debt_settlement_flag                0.314878\n",
      "7                 sub_grade                0.233739\n",
      "29          total_rec_prncp                0.233454\n",
      "6                     grade                0.229600\n",
      "4                  int_rate                0.211744\n",
      "34          last_pymnt_amnt                0.192632\n",
      "25                out_prncp                0.157300\n",
      "26            out_prncp_inv                0.157285\n",
      "27              total_pymnt                0.145531\n",
      "28          total_pymnt_inv                0.145372\n",
      "31       total_rec_late_fee                0.144559\n",
      "17           fico_range_low                0.122063\n",
      "18          fico_range_high                0.122062\n",
      "95               issue_year                0.112514\n",
      "11      verification_status                0.095424\n",
      "57     acc_open_past_24mths                0.094126\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pointbiserialr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "loan_data = \"data/accepted_2007_to_2018Q4.csv.gz\"\n",
    "unemployment_rate_data = [\"data/unemployment_rate_0.csv\", \"data/unemployment_rate_1.csv\", \"data/unemployment_rate_2.csv\", \"data/unemployment_rate_3.csv\", \"data/unemployment_rate_4.csv\"]\n",
    "\n",
    "data = preprocess_data(loan_data, unemployment_rate_data)\n",
    "target = data['default']\n",
    "features = data.drop(columns=['default'])\n",
    "numeric_cols = features.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "correlation_scores = []\n",
    "\n",
    "for col in numeric_cols:\n",
    "    try:\n",
    "        # Handle missing values in column by filling with median\n",
    "        series = features[col].fillna(features[col].median())\n",
    "        r, _ = pointbiserialr(target, series)\n",
    "        correlation_scores.append((col, abs(r)))\n",
    "    except Exception as e:\n",
    "        print(f\"Skipped {col} due to error: {e}\")\n",
    "\n",
    "corr_df = pd.DataFrame(correlation_scores, columns=['feature', 'abs_pointbiserial_corr'])\n",
    "corr_df = corr_df.sort_values('abs_pointbiserial_corr', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 Features by Point-Biserial Correlation:\")\n",
    "print(corr_df.head(20))\n",
    "\n",
    "# Save for audit\n",
    "corr_df.to_csv(\"pointbiserial_correlation_scores.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "riskBorrow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
