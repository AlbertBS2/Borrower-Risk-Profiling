{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25956c1b",
   "metadata": {},
   "source": [
    "## PCA on features with > 0.1 correlation according to pointBySerial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "931f9a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan data loaded successfully.\n",
      "Unemployment rate data loaded and merged successfully.\n",
      "Loan and unemployment data merged successfully.\n",
      "Data preprocessing completed successfully.\n",
      "Final data shape: (2260668, 19)\n",
      "   int_rate  grade  sub_grade  fico_range_low  fico_range_high  out_prncp  \\\n",
      "0     13.99      3         14           675.0            679.0        0.0   \n",
      "1     11.99      3         11           715.0            719.0        0.0   \n",
      "\n",
      "   out_prncp_inv   total_pymnt  total_pymnt_inv  total_rec_prncp  \\\n",
      "0            0.0   4421.723917          4421.72           3600.0   \n",
      "1            0.0  25679.660000         25679.66          24700.0   \n",
      "\n",
      "   total_rec_late_fee  recoveries  collection_recovery_fee  last_pymnt_amnt  \\\n",
      "0                 0.0         0.0                      0.0           122.67   \n",
      "1                 0.0         0.0                      0.0           926.35   \n",
      "\n",
      "   last_fico_range_high  last_fico_range_low  debt_settlement_flag  default  \\\n",
      "0                 564.0                560.0                     0        0   \n",
      "1                 699.0                695.0                     0        0   \n",
      "\n",
      "   issue_year  \n",
      "0        2015  \n",
      "1        2015  \n",
      "Numeric feature count: 18\n",
      "10 Explained variance ratio \n",
      "[0.24540545 0.21783364 0.13307511 0.10615284 0.06606898 0.06142098\n",
      " 0.05257269 0.04170157 0.04014693 0.02180257]\n",
      "10 Cum variance\n",
      "[0.24540545 0.46323909 0.59631419 0.70246703 0.76853602 0.829957\n",
      " 0.88252969 0.92423125 0.96437819 0.98618075]\n",
      "Print all features by PCA importance to -> pca_feature_importance_pointBiSerial.csv:\n",
      "Loadings for top features (first 5 PCs):\n",
      "                              PC1       PC2       PC3       PC4       PC5\n",
      "out_prncp               -0.170181 -0.216696 -0.361951  0.348960 -0.233851\n",
      "out_prncp_inv           -0.170174 -0.216690 -0.361951  0.348974 -0.233815\n",
      "last_fico_range_high    -0.298341  0.182882 -0.167438  0.109788  0.098008\n",
      "issue_year              -0.157617 -0.218250 -0.216285  0.185324 -0.147864\n",
      "fico_range_low          -0.311447  0.110522  0.121228  0.267244  0.473223\n",
      "fico_range_high         -0.311445  0.110521  0.121227  0.267245  0.473227\n",
      "last_fico_range_low     -0.256408  0.164490 -0.174911  0.107928  0.069540\n",
      "recoveries               0.180300 -0.092161  0.386246  0.423012 -0.064748\n",
      "collection_recovery_fee  0.175469 -0.089611  0.385302  0.429813 -0.076562\n",
      "total_pymnt_inv          0.132983  0.440492 -0.090125  0.177305 -0.166709\n",
      "Number of components to reach 95% variance: 9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from data_preprocessing import preprocess_data\n",
    "\n",
    "loan_data = \"data/accepted_2007_to_2018Q4.csv.gz\"\n",
    "unemployment_rate_data = [\"data/unemployment_rate_0.csv\", \"data/unemployment_rate_1.csv\", \"data/unemployment_rate_2.csv\", \"data/unemployment_rate_3.csv\", \"data/unemployment_rate_4.csv\"]\n",
    "\n",
    "data = preprocess_data(loan_data, unemployment_rate_data)\n",
    "\n",
    "\n",
    "low_pointBiSerial_corr_score_categories = [\"verification_status\", \"acc_open_past_24mths\", \"term\" ,\"num_tl_op_past_12m\",\"bc_open_to_buy\",\"inq_last_6mths\",\"y_unemployment_rate\",\"total_bc_limit\",\"percent_bc_gt_75\",\"bc_util\",\"revol_util\",\"tot_hi_cred_lim\",\"disbursement_method\",\"avg_cur_bal\",\"total_rev_hi_lim\",\"mo_sin_rcnt_tl\",\"mo_sin_rcnt_rev_tl_op\",\"mths_since_recent_inq\",\"tot_cur_bal\",\"all_util\",\"home_ownership\",\"open_acc_6m\",\"total_rec_int\",\"num_actv_rev_tl\",\"num_rev_tl_bal_gt_0\",\"hardship_flag\",\"mths_since_recent_bc\",\"il_util\",\"pymnt_plan\",\"max_bal_bc\",\"mort_acc\",\"open_rv_24m\",\"mths_since_rcnt_il\",\"mo_sin_old_rev_tl_op\",\"dti\",\"open_rv_12m\",\"installment\",\"application_type\",\"pub_rec\",\"inq_fi\",\"loan_amnt\",\"funded_amnt\",\"funded_amnt_inv\",\"inq_last_12m\",\"pub_rec_bankruptcies\",\"total_cu_tl\",\"num_op_rev_tl\",\"annual_inc\",\"num_actv_bc_tl\",\"revol_bal\",\"delinq_2yrs\",\"open_il_24m\",\"mo_sin_old_il_acct\",\"open_acc\",\"num_sats\",\"num_rev_accts\",\"purpose\",\"total_bal_il\",\"tax_liens\",\"total_acc\",\"num_tl_90g_dpd_24m\",\"open_act_il\",\"num_il_tl\",\"num_accts_ever_120_pd\",\"total_il_high_credit_limit\",\"collections_12_mths_ex_med\",\"total_bal_ex_mort\",\"num_bc_tl\",\"pct_tl_nvr_dlq\",\"emp_length\",\"acc_now_delinq\",\"num_tl_30dpd\",\"chargeoff_within_12_mths\",\"num_bc_sats\",\"delinq_amnt\",\"addr_state\",\"num_tl_120dpd_2m\",\"open_il_12m\", \"tot_coll_amt\"]\n",
    "data = data.drop(columns=low_pointBiSerial_corr_score_categories, errors='ignore')\n",
    "\n",
    "print(f'Final data shape: {data.shape}')\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(data.head(2))\n",
    "\n",
    "# Drop non-numeric columns and the target column `default` from features (but keep for reference)\n",
    "if 'default' in data.columns:\n",
    "    y = data['default']\n",
    "else:\n",
    "    y = None\n",
    "numeric_columns = data.columns[data.dtypes.apply(lambda x: np.issubdtype(x, np.number))]\n",
    "\n",
    "# ISSUES WITH MEMORY USAGE\n",
    "data = data[numeric_columns] \n",
    "# non_numeric = [c for c in data.columns if c not in numeric_columns]\n",
    "# data.drop(columns=non_numeric, inplace=True)\n",
    "\n",
    "# Remove IDs\n",
    "for col in ['id','member_id']:\n",
    "    if col in data.columns:\n",
    "        data = data.drop(columns=[col])\n",
    "# Ensure target not included in features\n",
    "if 'default' in data.columns:\n",
    "    data = data.drop(columns=['default'])\n",
    "\n",
    "print(f'Numeric feature count: {data.shape[1]}')\n",
    "\n",
    "# Impute missing values with median\n",
    "imp = SimpleImputer(strategy='median')\n",
    "X_imp = imp.fit_transform(data)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "# Run PCA\n",
    "batch_size = 50000\n",
    "for i in range(0, X_imp.shape[0], batch_size):\n",
    "    scaler.partial_fit(X_imp[i:i+batch_size])\n",
    "\n",
    "# Transform in batches\n",
    "X_scaled = np.zeros_like(X_imp, dtype=np.float32)  # Saves memory vs float64\n",
    "for i in range(0, X_imp.shape[0], batch_size):\n",
    "    X_scaled[i:i+batch_size] = scaler.transform(X_imp[i:i+batch_size])\n",
    "\n",
    "n_components = min(50, X_scaled.shape[1])  # Choose desired number of components (50 is typical)\n",
    "ipca = IncrementalPCA(n_components=n_components)\n",
    "\n",
    "# Fit in chunks\n",
    "for i in range(0, X_scaled.shape[0], batch_size):\n",
    "    ipca.partial_fit(X_scaled[i:i+batch_size])\n",
    "\n",
    "# Transform in chunks\n",
    "X_pca_list = []\n",
    "for i in range(0, X_scaled.shape[0], batch_size):\n",
    "    X_pca_list.append(ipca.transform(X_scaled[i:i+batch_size]))\n",
    "\n",
    "X_pca = np.vstack(X_pca_list)\n",
    "\n",
    "# Explained variance\n",
    "explained = ipca.explained_variance_ratio_\n",
    "cum_explained = np.cumsum(explained)\n",
    "print('10 Explained variance ratio ')\n",
    "print(explained[:10])\n",
    "print('10 Cum variance')\n",
    "print(cum_explained[:10])\n",
    "\n",
    "# Loadings DataFrame (features x components)\n",
    "loadings = pd.DataFrame(ipca.components_.T, index=data.columns, columns=[f'PC{i+1}' for i in range(ipca.n_components_)])\n",
    "importance = (loadings.abs() * explained).sum(axis=1)\n",
    "importance = importance.sort_values(ascending=False)\n",
    "impl_df = pd.DataFrame({'feature': importance.index, 'importance': importance.values})\n",
    "impl_df.to_csv('pca_feature_importance_pointBiSerial.csv', index=False)\n",
    "\n",
    "#print('Top 20 features by PCA importance:')\n",
    "# print(impl_df.head(20))\n",
    "print('Print all features by PCA importance to -> pca_feature_importance_pointBiSerial.csv:')\n",
    "\n",
    "top_feats = impl_df['feature'].head(10).tolist()\n",
    "print('Loadings for top features (first 5 PCs):')\n",
    "print(loadings.loc[top_feats, loadings.columns[:5]])\n",
    "\n",
    "# Components to reach 95% variance\n",
    "n_95 = np.searchsorted(cum_explained, 0.95) + 1\n",
    "print(f'Number of components to reach 95% variance: {n_95}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caec6871",
   "metadata": {},
   "source": [
    "### Logistic Regression on 19 features with highest correlation to target according to pointBySerial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e78e602a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 18 components for model input\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daniel\\anaconda3\\envs\\riskBorrow\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics:\n",
      "{'accuracy': 0.9412099067975424, 'precision': 0.7227530703996434, 'recall': 0.910368169060591, 'f1': 0.8057838860758568, 'roc_auc': 0.9774516170815488}\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.95      0.97    391564\n",
      "           1       0.72      0.91      0.81     60570\n",
      "\n",
      "    accuracy                           0.94    452134\n",
      "   macro avg       0.85      0.93      0.89    452134\n",
      "weighted avg       0.95      0.94      0.94    452134\n",
      "\n",
      "Confusion matrix:\n",
      "[[370412  21152]\n",
      " [  5429  55141]]\n",
      "Saved metrics to pca_logistic_metrics.csv and coefficients to pca_logistic_coefficients_pointBiSerial.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "desired = 51\n",
    "if 'n_95' in globals() and isinstance(n_95, int) and n_95 > 0:\n",
    "    n_use = min(desired, ipca.n_components_, n_95)\n",
    "else:\n",
    "    n_use = min(desired, ipca.n_components_)\n",
    "\n",
    "print(f'Using {n_use} components for model input')\n",
    "\n",
    "# Build feature matrix from PCA-transformed data X_pca\n",
    "if 'X_pca' not in globals():\n",
    "    X_pca = ipca.transform(X_scaled)\n",
    "X = X_pca[:, :n_use]\n",
    "\n",
    "try:\n",
    "    y = data['default'] \n",
    "except Exception:\n",
    "    # fall back to checking globals for y from earlier cells\n",
    "    if 'y' in globals() and y is not None:\n",
    "        pass\n",
    "    else:\n",
    "        raise RuntimeError('Target vector `y` not found in notebook namespace. Ensure you saved the original target before overwriting `data`.')\n",
    "\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "# Fit logistic regression with balanced class weights\n",
    "clf = LogisticRegression(max_iter=1000, class_weight='balanced', solver='saga')\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:, 1] if hasattr(clf, 'predict_proba') else None\n",
    "\n",
    "\n",
    "# Metrics\n",
    "metrics = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred),\n",
    "    'precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "    'recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "    'f1': f1_score(y_test, y_pred, zero_division=0),\n",
    "}\n",
    "\n",
    "if y_proba is not None:\n",
    "    metrics['roc_auc'] = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "print('Evaluation metrics:')\n",
    "print(metrics)\n",
    "\n",
    "print('Classification report:')\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Save metrics and model coefficients\n",
    "metrics_df = pd.DataFrame([metrics])\n",
    "metrics_df.to_csv('pca_logistic_metrics_pointBiSerial.csv', index=False)\n",
    "coef_df = pd.DataFrame({'component': [f'PC{i+1}' for i in range(n_use)], 'coef': clf.coef_.ravel()[:n_use]})\n",
    "coef_df.to_csv('pca_logistic_coefficients_pointBiSerial.csv', index=False) \n",
    "print('Saved metrics to pca_logistic_metrics_biSerial.csv and coefficients to pca_logistic_coefficients_pointBiSerial_biSerial.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187cbd41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "riskBorrow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
